---
title: "Birth Trends"
author: "Amber Thomas"
date: "April 5, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This is the script used to generate data for a story on Birth Trends in the US. 

### Loading Necessary Packages

```{r}
# For data sourcing and parsing
library(data.table)

# For data cleaning
library(tidyverse)
```

## Downloading Data
### 1968 - 2002

All of the data needed for this project can be found from the US Center for Disease Control and Prevention and National Center for Health Statistics' National Vital Statistics System (available [here](https://www.cdc.gov/nchs/data_access/vitalstatsonline.htm#Births)). However, the .zip folders for some of the older files download in a strange .PUB format that I couldn't open. Luckily, the National Bureau of Economic Research has made the microdata files available in several formats for download (available [here](http://www.nber.org/data/vital-statistics-natality-data.html)). 

I'll need the csv files. Since the files are listed as .csv.zip files, they'll also need to be unzipped on download. I'm going to create a function to download and unzip the csv files and turn each one into a data frame so I can loop through all 47 files automatically. Similarly, to cut down on file size, I'm only going to import the columns that I need: County, State, and Birth Month. These variables are all available on the individual-birth basis until 2006, but the column names change in 2003. For that purpose, I'll only download files from 1968 - 2002 using this loop.  I'll also consolidate the number of births per county per month. 

```{r}
# Function to download and unzip
dlUnzip <- function(year){
  URL <- "http://www.nber.org/natality/"
  
  # Columns to keep: County, State, 
  columns <- c("cntyres", "stateres", "birmon", "restatus")
  
  YearURL <- paste(URL, year, "/natl", year, ".csv.zip", sep = "")
  
  download <- fread(paste("curl", YearURL, "| funzip"), select = columns)
  
  # Keep US Residents
  
  download2 <- download %>% 
    # Remove any non-US residents
    filter(!restatus == 4) %>% 
    group_by(stateres, cntyres, birmon) %>% 
    summarise(Births = n()) %>% 
    # Adding a "Year" column to reflect the year variable fed into the function
    mutate(Year = year,
    # NCHS prohibits reporting geographic information of less than 9 births
    # values <= 9 are turned to NA for county level data       
      countyBirths = ifelse(Births <= 9, NA, Births)) %>% 
    # Monthly state-level births are then calculated
    group_by(stateres, birmon) %>% 
    mutate(stateBirths = sum(Births)) %>% 
    ungroup() #%>% 
    # Remove column of raw birth numbers to prevent reporting values < 9
    #select(-Births)
}
```

This function outputs a dataframe with 6 columns:
  * **stateres** : The state (including District of Columbia) where the mother lives (states are listed as numbers in alphabetical order such that Alabama is 1 and Wyoming is 51)
  * **cntyres** : The county where the mother lives
  * **birmon** : The month that the birth took place (1 is January and 12 is December)
  * **Year** : Year of the birth, obtained from the year of the data file itself
  * **countyBirths** : The calculated sum of births that occurred to mothers living in a county for a given month (if the sum was less than 9, the sum is listed as NA as per NCHS reporting guidelines)
  * **stateBirths** : The calculated sum of births that occurred to mothers living in a state for a given month
  
I'm going to run through this function for each year between 1968 and 2002, binding each year's data together.

```{r}
dataList <- c(1968:2002)
nchsData <- do.call(rbind, (lapply(dataList, (dlUnzip))))
```

The county codes listed in these files are the NCHS county codes. However, the codes used in the CDC WONDER tool (needed for the 2003 - 2015 data) are the FIPS (Federal Information Processing Standards) county codes. Luckily, the FBER provides a file to walk users between the two county codes. For ease, I'm going to convert the NCHS codes used in the newly acquired nchsData file to FIPS codes. 

```{r}
# Downloading walk file
walk <- read.csv("http://www.nber.org/mortality/nchs2fips_county1990.csv", header = TRUE)
```

The NCHS county codes in the nchsData file are made up of a 2 digit state code and 3 digit county code. I'll add some zeros for padding to the walk data frame and paste the two values together. 

```{r}
walk2 <- walk %>% 
  mutate(nchs_state = sprintf("%02d", nchs_state),
         nchs_county = sprintf("%03d", nchs_county),
         nchs_county_code = paste(nchs_state, nchs_county, sep = ""))
```

Now it's just a matter of matching the county codes that are already in the nchsData file with the matching FIPS county code. 

```{r}
#Make a copy of the dataset
nchsData2 <- nchsData

# Match the NCHS County Codes
nchsData2$FIPSCounty <- walk2[match(nchsData2$County, walk2$nchs_county_code), "fipsco"]
```

For safe-keeping, I'm going to write the data into a csv file with both the NCHS and FIPS county codes. 
```{r}
write.csv(nchsData2, "../Raw_Data/birthData.csv")
```


### 2003 - 2015

For privacy reasons, NCHC doesn't release geolocation information for individual babies born after 2005. They do, however, have an interactive tool called [WONDER](https://wonder.cdc.gov/natality.html) which allows you to download specific aggregate data. I'm going to group data from this database the same way I grouped data from the raw microdata files above. For both the 2003 - 2006 database and the 2007 - 2015 database, I selected data by Year, Month, State, and County, further selecting for each individual year. I then exported the data as a text file. 

*Note: In the exported text file, there is metadata at the bottom of the file, I removed this before importing the file into R, and I changed the column named "Births" to "countyBirths" to match the 1968 - 2002 data*

Now the resulting text file can be imported into R. 

```{r}
wonderData <- read.table("WONDER_Data/Natality_2003_2015.txt", fill = TRUE, header = TRUE)
```

To match the WONDER data with the NCHS data, the columns can be reduced to just include the Year, Month_Code, County_Code, State_Code, and countyBirths. Then I'll calculate stateBirths the same way I did for the NCHS data. 

```{r}
wonderData2 <- wonderData %>% 
  select(Year_Code, Month_Code, County_Code, State_Code, countyBirths) %>% 
  group_by(State_Code, Year_Code, Month_Code) %>% 
    mutate(stateBirths = sum(countyBirths)) %>% 
    ungroup()
```

Now the columns in the two files need to be renamed to match. For ease, I'll use the following column names:

  * **State** : The state (including District of Columbia) where the mother lives (states are listed as numbers in alphabetical order such that Alabama is 1 and Wyoming is 51)
  * **County** : The county where the mother lives (FIPS county code)
  * **Month** : The month that the birth took place (1 is January and 12 is December)
  * **Year** : Year of the birth, obtained from the year of the data file itself
  * **countyBirths** : The calculated sum of births that occurred to mothers living in a county for a given month (if the sum was less than 9, the sum is listed as NA as per NCHS reporting guidelines)
  * **stateBirths** : The calculated sum of births that occurred to mothers living in a state for a given month
  
  
```{r}
# Removing the NCHS county code 
nchsData3 <- nchsData2 %>% 
  select(-County)

nchsCol <- c("Year", "Month", "FIPSCounty", "State", "countyBirths", "stateBirths")
wonderCol <- c("Year_Code", "Month_Code", "County_Code", "State_Code", "countyBirths", "stateBirths" )
newColnames <- c("Year", "Month", "County", "State", "countyBirths", "stateBirths")

setnames(nchsData3, old = nchsCol, new = newColnames)
setnames(wonderData2, old = wonderCol, new = newColnames)
```

Last thing to do is bind the 1968 - 2002 data to the 2003 - 2015 data. 

```{r}
allBirthData <- rbind(nchsData3, wonderData2)
```

And export to CSV. 

```{r}
write.csv(allBirthData, "../Raw_Data/allBirthData.csv")
```

## Preparing for d3

If I were going to continue to use this data only in R, I'd leave it organized as is, however, since I am planning to feed the data into [d3](https://d3js.org/), I'm going to make a month-year column (%m-%Y). This will hopefully make the d3 run a little smoother. 


```{r}
allBirthData2 <- allBirthData %>% 
  mutate(monthYear = paste((Month), Year, sep = "-")) %>% 
  select(-Month, -Year) %>% 
  filter(!is.na(countyBirths))
```

Again, I'll write a 3rd raw data file, specifically used for javascript-things.

```{r}
write.csv(allBirthData2, "../Raw_Data/jsBirthData.csv")
```


For ease of prototyping, I'm also going to make a state/region data file. 

```{r}
states <- state.name
div <- state.division
DC <- c("District of Columbia", "5")

states2 <- as.data.frame(cbind(states, div), stringsAsFactors = FALSE)
states2 <- rbind(states2, DC)

states2 <- states2 %>% arrange(states) %>% mutate(number = c(1:51), number = sprintf("%02d", number))

stateData <- allBirthData2 %>% 
  mutate(number = State, number = sprintf("%02s", number)) %>%  
  group_by(number, monthYear) %>% 
  summarize(stateBirths = mean(stateBirths)) 

stateData2 <- left_join(stateData, states2, by = "number")

write.csv(stateData2, "../d3Graphics/Exploration3/stateData.csv")
```

```{r}
divs = stateData2 %>% 
  separate(monthYear, into = c("month", "year"), sep = "-") %>% 
  mutate(month = as.numeric(month)) %>% 
  group_by(year, month, div) %>% 
  summarize(sum = sum(stateBirths)) %>% 
  filter(!is.na(div))
  

ggplot(divs, aes(x = month, y = sum, group = year, col = year)) + geom_line() + facet_wrap(~div)
```

```{r}
divs2 = stateData2 %>% 
  separate(monthYear, into = c("month", "year"), sep = "-") %>% 
  mutate(month = as.numeric(month),
         State = as.factor(states)) %>% 
  group_by(year, month, div, State) %>% 
  summarize(sum = sum(stateBirths)) %>% 
  filter(!is.na(State)) %>% 
  ungroup
  

ggplot(divs2, aes(x = month, y = sum, group = year, color = div)) + geom_line() + facet_wrap(~State)


##+ facet_grid(~states)
```

## Reducing CSV Size

The CSV file I was loading into d3 was 28 MB which was really slow to load in d3. I decided to cut down the file size by doing the following:

* Limit the file to only counties that have data all the way to 2015 (this limits to big cities with full data sets)
* Cut the data file to data collected after 1985 (when all states were reporting 100% of births)
* Limit the date format to yymm (or %y%m) to save space
* Round the county births to the nearest 10 and divide the number by 10 (it can be multiplied by 10 in d3, bu this saves characters with little impact on precision)
* Eliminate information about the state and births within the state. These can be dealt with in another file

```{r}
fifteenonly <- allBirthData %>% 
  filter(Year == 2015) %>% 
  select(6) 

condensed2 <- allBirthData %>% 
  mutate(complete = County %in% fifteenonly$County) %>% 
  filter(complete == TRUE,
         Year >= 1995) %>% 
  mutate(Yr = str_sub(Year, start= -2),
         M = sprintf("%02d", Month)) %>% 
  mutate(Date = paste0(Yr, M),
         ## round to nearest 5 and eliminate last 0 for space
         Births = (round(countyBirths, -1))/10) %>% 
  select(c(6, 10, 11)) 

write.csv(condensed2, "../Raw_Data/CondensedBirth2.csv")
```

After all of that, the file size is reduced to 4.6 MB or an 84% reduction in file size. Not bad! 

One option to limit the dataset a little further is to only include counties for which we have all 30 years (1985 - 2015) of data. 

```{r}

all20 <- allBirthData %>% 
  mutate(complete = County %in% fifteenonly$County) %>% 
  filter(complete == TRUE,
         Year >= 1995) %>%
  select(c(3, 6)) %>% 
  group_by(County) %>% 
  summarize(years = n_distinct(Year)) %>% 
  filter(years >= 20)

condensed3 <- condensed2 %>% 
  mutate(complete = County %in% all20$County) %>% 
  filter(complete == TRUE) %>% 
  select(-4)

  
write.csv(condensed3, "../Raw_Data/CondensedBirth3.csv")
```  

## Making Lookup table for county and state names
```{r}
countyNames <- read.csv("../Raw_Data/countyNames.csv", header = TRUE, stringsAsFactors = FALSE)
stateNames <- read.csv("../Raw_Data/stateNames.csv", header = TRUE, stringsAsFactors = FALSE)
```

```{r}
condensedCN <- countyNames %>% 
  mutate(complete = County %in% all20$County) %>% 
  filter(complete == TRUE) %>% 
  select(-4)

stateCounty = left_join(condensedCN, stateNames, by = "State") %>% 
  select(-1)

write.csv(stateCounty, "../Raw_Data/stateCountyNames.csv")
```

```{r}
stateMean = read.csv("../Raw_Data/stateData.csv", header = TRUE, stringsAsFactors = FALSE)

stateData2 <- allBirthData %>% 
  mutate(number = State, number = sprintf("%02s", number)) %>%  
  filter(Year >= 1995) %>% 
  mutate(Yr = str_sub(Year, start= -2),
         M = sprintf("%02d", Month), 
         Date = paste0(Yr, M))
  group_by(number, Date) %>% 
  summarize(stateBirths = mean(stateBirths)) 
  


stateData2 <- left_join(stateData, states2, by = "number")

write.csv(stateData2, "../d3Graphics/Exploration3/stateData.csv")
```